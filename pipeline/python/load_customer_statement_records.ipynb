{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import round\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "import os \n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# SECRETS\n",
        "connection_string = \"**********\"\n",
        "jdbc_username = \"**********\"\n",
        "jdbc_password = \"**********\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define schema of source\n",
        "schema = StructType() \\\n",
        "    .add(\"Reference\", IntegerType(), True) \\\n",
        "    .add(\"Account Number\", StringType(), True) \\\n",
        "    .add(\"Description\", StringType(), True) \\\n",
        "    .add(\"Start Balance\", FloatType(), True) \\\n",
        "    .add(\"Mutation\", FloatType(), True) \\\n",
        "    .add(\"End Balance\", FloatType(), True)\n",
        "\n",
        "try:\n",
        "    # load source into dataframe\n",
        "    df = spark.read.load(\n",
        "        'abfss://codingassignmentfilesystem@codingassignmentstorage.dfs.core.windows.net/arriving_data/*', \n",
        "        format='csv', \n",
        "        header=True,\n",
        "        delimiter=\",\",\n",
        "        schema=schema\n",
        "    )\n",
        "except AnalysisException: \n",
        "    print(\"No data found in path\")\n",
        "    mssparkutils.session.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def convert_cols_snakecase(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Converts all column names in the provided dataframe to snakecase\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: DataFrame\n",
        "        The DataFrame for which the columns should be converted to snakecase\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    df: DataFrame\n",
        "    \"\"\"\n",
        "    new_column_names = [col.replace(\" \", \"_\").lower() for col in df.columns]\n",
        "    return df.toDF(*new_column_names)\n",
        "    \n",
        "    # col_mapping ={}\n",
        "    # for col_name in df.columns:\n",
        "    #     new_col_name = col_name.replace(\" \", \"_\").lower()\n",
        "    #     col_mapping[col_name] = new_col_name\n",
        "    # for column_name in df.columns:\n",
        "    #     df= df.withColumnRenamed(column_name, col_mapping[column_name])\n",
        "    # return df\n",
        "\n",
        "# convert column names to snake case \n",
        "df = convert_cols_snakecase(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def find_duplicate_column_values(df: DataFrame, column: str) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Finds values for provided column in provided dataframe for which duplicate values exist\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: DataFrame\n",
        "        The DataFrame that should be searched for duplicate values on provided column\n",
        "    column: str \n",
        "        The column that should be searched for duplicate values  \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    duplicate_references: DataFrame\n",
        "    \"\"\"\n",
        "    return df.groupBy(column).count().where('count > 1').drop('count')\n",
        "\n",
        "# get dataframe in which duplicate values for reference are stored\n",
        "duplicate_references = find_duplicate_column_values(df, \"reference\")\n",
        "duplicate_references.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def deduplicate_rows_on_column(df: DataFrame, duplicate_reference_df: DataFrame) -> (DataFrame, DataFrame):\n",
        "    \"\"\"\n",
        "    Removes duplicate records from provided dataframe: df, using the values in provvided dataframe: duplicate_reference_df\n",
        "    Returns a dataframe from which the duplicates are removed and returns a dataframe containing the duplicate records\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: DataFrame\n",
        "        The DataFrame from which the duplicate values should be removed\n",
        "    duplicate_reference_df: DataFrame\n",
        "        The DataFrame from which the values will be used to determine duplicate records\n",
        "        This DataFrame should contain only one column in which the duplicate values are stored \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (not_duplicate_records: DataFrame, duplicate_records: DataFrame) \n",
        "    \"\"\"\n",
        "    not_duplicate_records = df.filter(~df['reference'].isin([ int(row['reference']) for row in duplicate_reference_df.collect()]))\n",
        "    duplicate_records = df.filter(df['reference'].isin([ int(row['reference']) for row in duplicate_reference_df.collect()]))\n",
        "    return not_duplicate_records, duplicate_records\n",
        "\n",
        "# get df (dataframe without any duplicate rows) and filtered_df (dataframe containing the duplicate rows)\n",
        "df, filtered_df = deduplicate_rows_on_column(df, duplicate_references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def round_numerical_columns(df: DataFrame, columns: list, decimals: int) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Rounds all the columns provided in the column (list) parameter in the provided dataframe df to the amount of decimals in the provided decimals int\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    df: DataFrame\n",
        "        The DataFrame for which the column values should be rounded \n",
        "    columns: list \n",
        "        A list of columns that should be rounded \n",
        "    decimals: int \n",
        "        The amount of decimals the numerical values should be rounded to \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    df: DataFrame \n",
        "    \"\"\"\n",
        "    for column in columns: \n",
        "        df = df.withColumn(column, round(df[column], 2))\n",
        "    return df \n",
        "\n",
        "# round numerical columns start_balance, mutation, and end_balance to two decimals\n",
        "df = round_numerical_columns(df, [\"start_balance\", \"mutation\", \"end_balance\"], 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def check_balance_after_mutation(df: DataFrame, col_start_balance: str, col_mutation: str, col_end_balance: str) -> (DataFrame, DataFrame):\n",
        "    \"\"\"\n",
        "    Checks if balances are correct after mutation by checking whether start_balance + mutation == end_balance\n",
        "    Correct and incorrect transactions are split into separate DataFrames which are returned by the function \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: DataFrame \n",
        "        The DataFrame for which the balances should be checked \n",
        "    col_start_balance: str \n",
        "        The column that contains the start_balance \n",
        "    col_mutation: str \n",
        "        The column that contains the mutation \n",
        "    col_end_balance: str \n",
        "        The column that contains the end_balance\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df_correct_balance: DataFrame \n",
        "        The DataFrame containing the correct end_balance \n",
        "    df_incorrect_balance: DataFrame \n",
        "        The DataFrame containing the incorrect end_balance\n",
        "    \"\"\"\n",
        "    df_correct_balance = df.filter(round((df[col_start_balance] + df[col_mutation]),2) == df[col_end_balance])\n",
        "    df_incorrect_balance = df.filter(round((df[col_start_balance] + df[col_mutation]),2) != df[col_end_balance])\n",
        "    return df_correct_balance, df_incorrect_balance\n",
        "\n",
        "# get df with correct balances and get df_incorrect_balance with incorrect balances\n",
        "df, df_incorrect_balance = check_balance_after_mutation(df, \"start_balance\", \"mutation\", \"end_balance\")\n",
        "\n",
        "# append the incorrect_balance dataframe to the filtered_df dataframe\n",
        "filtered_df = filtered_df.union(df_incorrect_balance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def generate_report_content(filtered_df: DataFrame) -> str: \n",
        "    \"\"\"\n",
        "    Generates report_content for transactions that did not pass validations. Will output reference and description for each failed transaction. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filtered_df: DataFrame \n",
        "        The DataFrame containing rows that did not pass validation. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    report_content: str\n",
        "    \"\"\"\n",
        "\n",
        "    filtered_transaction_array = filtered_df.toPandas().to_dict(\"records\")\n",
        "    report_header = \"Transactions that did not pass validation:\\n------------------------------------------\\n\"\n",
        "    report_line_array = [f\"reference: {transaction['reference']}; description: {transaction['description']}\" for transaction in filtered_transaction_array]\n",
        "    report_lines = \"\\n\".join(report_line_array)\n",
        "    report_content = report_header + report_lines\n",
        "    return report_content\n",
        "\n",
        "# generate report_content from filtered_df DataFrame\n",
        "report_content = generate_report_content(filtered_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def upload_report_to_adls(storage_account_name: str, file_system_name: str, destination_dir: str, destination_path: str, connection_string: str, report_content: str) -> None:\n",
        "    \"\"\"\n",
        "    Uploads the pdf report to adls\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    storage_account_name: str \n",
        "        The storage account on which the adls filesystem resides \n",
        "    file_system_name: str \n",
        "        The name of the adls filesystem\n",
        "    destination_dir: str\n",
        "        The directory in the filesystem to store the pdf to \n",
        "    destination_path: str \n",
        "        The filepath to store the pdf to\n",
        "    connection_string: str \n",
        "        Storage account key used for authentication \n",
        "    report_content: \n",
        "        The content that should be written to adls \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None \n",
        "    \"\"\"\n",
        "    service_client = DataLakeServiceClient.from_connection_string(conn_str=connection_string)\n",
        "    file_system_client = service_client.get_file_system_client(file_system=file_system_name)\n",
        "    directory_client = file_system_client.get_directory_client(os.path.dirname(destination_dir))\n",
        "    file_client = directory_client.get_file_client(os.path.basename(destination_path))\n",
        "\n",
        "    file_client.upload_data(report_content, overwrite=True)\n",
        "\n",
        "# set required variables for uploading the pdf to adls\n",
        "storage_account_name = \"codingassignmentstorage\"\n",
        "file_system_name = \"codingassignmentfilesystem\"\n",
        "destination_dir = f\"/reports/\"\n",
        "destination_path = f\"/reports/failed_transactions_{str(time.time())}\"\n",
        "\n",
        "# upload the pdf to adls \n",
        "upload_report_to_adls(storage_account_name, file_system_name, destination_dir, destination_path, connection_string, report_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def write_df_to_db(jdbc_hostname: str, jdbc_database: str, jdbc_username, jdbc_password, df: DataFrame, table_name: str, mode: str=\"ignore\", jdbc_port: int=1433) -> None:\n",
        "    \"\"\"\n",
        "    Writes a dataframe to a database table \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    jdbc_hostname: str \n",
        "        The hostname of the sql server \n",
        "    jdbc_database: str \n",
        "        The name of the sql database \n",
        "    jdbc_username: str \n",
        "        The username for connecting to the database \n",
        "    jdbc_password: str \n",
        "        The password for connecting to the database \n",
        "    df: DataFrame \n",
        "        The DataFrame to write to the database \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    \"\"\"\n",
        "    jdbc_connection_string = f\"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};database={jdbc_database}\"\n",
        "    connection_properties = {\n",
        "        \"user\": jdbc_username,\n",
        "        \"password\": jdbc_password,\n",
        "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "    } \n",
        "    df.write.jdbc(url=jdbc_connection_string, table=table_name, mode=mode, properties=connection_properties)\n",
        "\n",
        "\n",
        "# set the variables required to write the dataframes to the azure sql database \n",
        "jdbc_hostname = \"codingassignmentsqlserver.database.windows.net\"\n",
        "jdbc_database = \"codingassignmentdb\"\n",
        "\n",
        "# write transactions to transactions table \n",
        "write_df_to_db(jdbc_hostname, jdbc_database, jdbc_username, jdbc_password, df, \"transactions\")\n",
        "\n",
        "# write transactions that did not pass validation to the failed_transactions table \n",
        "write_df_to_db(jdbc_hostname, jdbc_database, jdbc_username, jdbc_password, filtered_df, \"failed_transactions\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
